\ifcase 1  % choose 0=slides, 1=article, 2=refart
   \documentclass[aspectratio=169,ignorenonframetext,9pt]{beamer}
\or\documentclass[a4paper,11pt]{article}
   \usepackage{url,beamerarticle}
\or\documentclass[a4paper,11pt]{refart}
   \let\example\relax
   \usepackage{url,beamerarticle}
\fi

\ifcase 0  % choose a theme like these
    % \usetheme{boxes}
    \usetheme{Boadilla}
    % \usetheme{Goettingen}% I recommend
    % \usetheme{Singapore}
    % \usetheme{Pittsburgh}
    % \usetheme{Madrid}
    % \usetheme{Warsaw} % common choice, but often poor
\fi

\usepackage{algorithm,algorithmicx}
\usepackage{graphicx,pgfplots,parskip}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,url,array}
\usepackage{cite}




\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{algo}{Algorithm}[section]


\title{BSTERGM: Bayesian Separable-Temporal Exponential family Graph Models}
\author{Choi, Seokjun}
\date{14 Dec. 2020}


\begin{document}

\begin{frame}
\maketitle
\end{frame}


\begin{abstract}
Separable Temporal Exponential Random Graph Models (STERGM) are useful models to describe changes in dynamic networks' structures evolving over time. 
The Bayesian-inference based approach for the model, however, had not been developed yet. 
In this paper, I propose the Bayesian STERGM (BSTERGM): STERGM with the Bayesian setting, and I illustrate the computational algorithm that generates posterior samples from the model dealing with the doubly-intractable constant problem. 
Furthermore, I show the interpretability of the model in the Bayesian context.

\end{abstract}


% \begin{frame}{Outline}
% \tableofcontents
% \end{frame}

\section{Introduction}


\section{Background}
\subsection{Random graphs}
For statistical analysis on relations between subjects of data,
I will introduce terminologies and notations for denoting components related to networks, 
which are a representation of these type of data.

We say each subject on the data to a node. They are same as a vertex in graph theory.
In addition, we say a tie joining two nodes to an edge. A tie can be directed or not.
These two components have fundamental role describing the data.
Define a graph or a network to $y=(V,E)$, where $V$ is a set of node and $E$ is a set of edges of a network.

For simplicity, I will deal with a situation without weight on each node or edge.
Then, a graph $y$ can be represented by simpler form.
Assume that the number of node $n\in \mathbb{N}$ on data is given.
Then, for $\mathcal{B}=\{0,1\}$,
the set $\mathcal{Y} \subset \mathcal{B}^{n^2}$ is a set of graphs of $n$ nodes.
Now, a graph $y \in \mathcal{Y}$ can be represented by a $n \times n$ binary matrix
with elements $y_{ij}=1$ if there is a tie from $i$-th node to $j$-th node,
otherwise $y_{ij}=0$.

If edges of $y$ have directions, then $y$ is called a directed graph. Otherwise, $y$ is called a undirected graph.
These are obvious that, for given $n\in \mathbb{N}$,
$|\mathcal{Y}|=2^{n(n-1)}$ if $\mathcal{Y}$ is the set of all directed graphs not permitting self-connecting edges,
and $|\mathcal{Y}|=2^{n(n-1)/2}$ if $\mathcal{Y}$ is the set of all graphs of undirected one.
Thus, the size of $\mathcal{Y}$ grows exponentially when n increases.

I extend the concept of graphs to be able to describe a situation that connections among nodes are random.
Let $\Omega$ be an event set of all possible connected situations among given nodes.
We say $Y: \Omega \to \mathcal{Y}$ is a random variable for a graph, or a random graph.
For a random graph $Y \in \mathcal{Y}$, denote the edge between i-th node and j-th node by $Y_{ij}$ for $i,j=1,2,...,n$
satisfying $Y_{ij}=1$ if the edge is connected. Otherwise, $Y_{ij}=0$.
Note that the $Y_{ij}$s are also random variables.

Let me notate a realization of random graph by $y$ and its edges by $y_{ij}$ for $i,j=1,2,...,n$.


\subsection{Exponential random graph model: ERGM}
From the definition of random graph, one of the natural questions is 
how we can model a distribution on $\mathcal{Y}$.
As a answer, the Exponential random graph models (ERGM) (Wasserman and Pattison(1996) and Robins et al.(2007)) %ref
gives a classical but 
flexible scheme to model for time-invariant networks represented as a single directed or undirected graph.
Moreover, ERGM provide a statistical approach to the systematic exploration of several local features simultaneously and 
how these local structures interact to form a global network.

Specifically, following the notation of previous section,
let $\mathcal{Y}$ be a set of random graphs with $n$ nodes and $Y \in \mathcal{Y}$ be a random graph.
Set a distribution on $\mathcal{Y}$ to
    \[P(Y=y;\theta) = \frac{exp(\theta^{T}s(y))}{c(\theta)}\]
for some $\theta\in\mathbb{R}^p$,
where $s(y)\in\mathbb{R}^p$ is a vector which is part of y's sufficient network statistics,
and $c(\theta)\in\mathbb{R}$ is a normalizing constant satisfying $c(\theta)=\sum_{y\in\mathcal{Y}}exp(\theta^{T}s(y))$.
Models which have such a form called the ERGM.

Note that the form of the right-hand side is the canonical form of exponential family distributions.
That is why the model is called exponential. Moreover, observe that the normalizing constant cannot be directly calculated
because the size of $\mathcal{Y}$ grows exponentially as $n$ grows.
This situation is said to doubly-intractable constant problem, and it causes serious challenge to estimate the parameter vector $\theta$
in real data analysis. For more detail on fitting algorithm using MCMCMLE, see Hunter and Handcock(2006).% ref

\subsection{BERGM: Bayesian ERGM}
Caimo and Friel(2012) %ref
extend ERGM to bayesian setting. Putting prior $p(\theta)$ to the parameter $\theta$, the model becomes
\[p(\theta|y)=\frac{p(y|\theta)p(\theta)}{p(y)}=\frac{exp(\theta^T s(y))}{c(\theta)}\frac{p(\theta)}{p(y)}\]
Since not only the doubly-intractable constant problem still exists but also the $p(y)$ cannot be evaluated directly,
the estimation of $\theta$ needs more advanced algorithm than the standard MCMC procedure.
The solution of Caimo and Friel is using the exchange algorithm.

%exchange algorithm 설명


\subsection{TERGM: Temporal Exponential family Random Graphs Models}
Let's consider a sequence of random graph $\{Y_1, Y_2, ..., Y_T\} (T\in\mathbb{N})$ and assume that
we are interested in the dynamics or evolution of the network sequence.
The temporal Temporal Exponential family Random Graphs Models(TERGM)
proposed by Hanneke, Fu and Xing(2009)%ref
is a model for our interest if we permit a Markov assumption
\[P(Y_2,Y_3,...,Y_T|Y_1)=P(Y_2|Y_1)P(Y_3|Y_2)...P(Y_T|Y_{T-1})\]
and focus on the transition probability along index of the sequence.
For convenience, I refer the index as a time.

Specifically, let $\mathcal{Y}$ be a set of graphs with $n$ nodes. 
Let $Y_1=y_1 \in \mathcal{Y}$ be given and $Y_2,...,Y_T (T\in\mathbb{N})$ be random graphs in $\mathcal{Y}$.
Set a distribution on $\mathcal{Y}\times ... \times \mathcal{Y}$ ($T-1$ folds) to
\[P(Y_t=y_t|Y_{t-1}=y_{t-1};\theta) = \frac{exp(\theta^{T}s(y_t, y_{t-1}))}{c(\theta, y_{t-1})}\]
where $t=2,...,T$, $\theta\in\mathbb{R}^p$,
sufficient network statistics $s(y_t, y_{t-1})\in\mathbb{R}^n$,
and a normalizing constant $c(\theta, y_{t-1})=\sum_{y\in\mathcal{Y}}exp(\theta^{T}s(y, y_{t-1}))$.
Models which have a such form called the TERGM.

The estimation of $\theta$ can be performed by MCMCMLE,
but the estimation suffers severe degeneracy problem.
Furthermore, When we interpret the estimated TERGM parameters,
we cannot separate the generation and duration because
the higher value of $\theta$ increases both.
These raise demand for modified models to remedy the degeneracy and enable separable interpretation.

\subsection{STERGM: Separable-Temporal Exponential family Random Graphs Models}
\begin{defn}[STERGM: Separable-Temporal Exponential family Random Graphs Models]
    Let $\mathcal{Y}$ be a set of graphs with $n$ nodes. 
    Let $Y_1=y_1 \in \mathcal{Y}$ be given and $Y_2,...,Y_T (T\in\mathbb{N})$ be random graphs (on $\mathcal{Y}$).
    Set a distribution on $\mathcal{Y}\times ... \times \mathcal{Y}$ ($T-1$ folds) by following way:

    Let $\mathcal{Y}^+|_t$ be a subset of $\mathcal{Y}$ consisting all graphs which have equal or additional edges comparing to $y_{t-1}$.
    Likewise, let $\mathcal{Y}^-|_t$ be a subset of $\mathcal{Y}$ consisting all graphs which have equal or sparse edges comparing to $y_{t-1}$.
    Next, for $Y_t^+: \Omega \to\mathcal{Y}^+|_t$, $Y_t^-: \Omega \to\mathcal{Y}^-|_t$, $y_t^+ \in \mathcal{Y}^+|_t$ and $y_t^- \in \mathcal{Y}^-|_t$, set
    \[P(Y_t^+=y_t^+|Y_{t-1}=y_{t-1};\theta^+) = \frac{exp((\theta^+)^{T}s(y_t^+, y_{t-1}))}{c(\theta^+, y_{t-1})}\]
    \[P(Y_t^-=y_t^-|Y_{t-1}=y_{t-1};\theta^-) = \frac{exp((\theta^-)^{T}s(y_t^-, y_{t-1}))}{c(\theta^-, y_{t-1})}\]
    for some $\theta^+,\theta^-\in\mathbb{R}^p$, $s(y_t^+, y_{t-1}), s(y_t^-, y_{t-1})\in\mathbb{R}^n$, which are parts of sufficient network statistics,
    and normalizers $c(\theta^+, y_{t-1})=\sum_{y^+\in\mathcal{Y}^+}exp((\theta^+)^{T}s(y^+, y_{t-1})), c(\theta^-, y_{t-1})=\sum_{y^-\in\mathcal{Y}^-}exp((\theta^-)^{T}s(y^-, y_{t-1}))$.
    
    Then, defining operations $+,-$ on $\mathcal{Y}$ following the boolean algebra edgewise-ly, set $y_t$ to
    \[y_t=y_t^+ - (y_{t-1} - y_t^-) = y_t^- + (y_t^+ - y_{t-1})\]
    
    Additionally, assume that
    \begin{itemize}
        \item The first-order Markov assumption: $P(Y_2,...,Y_T|Y_1)=P(Y_2|Y_1)...P(Y_T|Y_{T-1})$
        \item The separability: the conditional independence between $Y_t^+$ and $Y_t^-$ for all $t=2,...,T$;
            thus, \(P(Y_t=y_t|Y_{t-1}=y_{t-1};\theta^+,\theta^-)=P(Y_t^+=y_t^+|Y_{t-1}=y_{t-1};\theta^+)P(Y_t^-=y_t^-|Y_{t-1}=y_{t-1};\theta^-)\)
    \end{itemize}
    Models which have a such form called the STERGM.
\end{defn}


\subsection{Network Statistics for BSTERGM}
As a usual, $s(y_{t}^.,y_{t-1})$ would be chosen by a difference of network statistics, $s'(y_t^.)-s'(y_{t-1})$.
A common candidates of $s'$ are:
\begin{itemize}
    \item the number of edges
    \item node degree distribution (each order)
    \item edgewise shared partner distribution (each order)
    \item dyadwise shared partner distribution (each order)
    \item k-star distribution
    \item triangle distribution
\end{itemize}
In fact, a bundle of all order of node degree distribution and all order of edgewise shared partner distribution form sufficient statistics of a graph.
Also, we can set $s'$ by a function of these statistics.


\section{Bayesian STERGM}

To convert the STERGM to the Bayesian setting, put priors $p(\theta^+),p(\theta^-)$ over $\theta^+,\theta^-$ and take the Bayes theorem as a inference rule.
Then, the posterior of $\theta^+,\theta^-$ becomes
\[P(\theta^+,\theta^-|y_t, y_{t-1}) = \frac{P(Y_t^+=y_t^+|y_{t-1},\theta^+) P(Y_t^-=y_t^-|y_{t-1},\theta^-)P(\theta^+),P(\theta^-)}{c(\theta^+,y_{t-1})c(\theta^-,y_{t-1})} \]
where $y_t=y_t^+ - (y_{t-1} - y_t^-) = y_t^- + (y_t^+ - y_{t-1})$.

Remarks:
\begin{itemize}
    \item We cannot compute the normalizing constants $c(\theta^+,y_{t-1})$, $c(\theta^-,y_{t-1})$ practically because we need to sum up too many terms.
    \item The constants are doubly intractable: they depend on $\theta^+$,$\theta^-$, the parameters of a model.
        Thus, we cannot use an ordinary MCMC algorithm to get the posterior sample.
\end{itemize}


\section{Bayesian Estimation}

Suppose that a sequence of graph samples $y_1,...,y_T$ is observed
\\ (then, $y_2^+,...,y_T^+$ and $y_2^-,...,y_T^-$ are uniquely determined,) 
\\ and we want to fit the observation using BSTERGM.

How do we find the posterior distribution of $\theta^+,\theta^-$?

Note that an ordinary MCMC algorithm does not work in our situation, because the constant part remains when calculating the MCMC ratio.
To solve this doubly-intractable constant problem, we should use more special MCMC technique, the exchange MCMC algorithm.

Here is the algorithm of our main chain for the exchange algorithm.
\begin{algo}[the main chain]
Let $y_1,...,y_T$ be given. For $m=1,...,M$,
\begin{enumerate}
    \item Propose candidates $\theta_*^+,\theta_*^-$ from $\epsilon(.|\theta_{m-1}^+,\theta_{m-1}^-)$.
    \item Select a lag $(t-1,t)$ randomly on $2 \leq t \leq T$.
    \item Generate an exchange graph $y_{ex} \in\mathcal{Y}|_t$ (with $y_{ex}^+, y_{ex}^-$) at the $\theta_*^+,\theta_*^-$.
    \item Calculate the exchange MCMC ratio $\pi$ at the lag,
        \[\pi = \frac{P(y_t^+|y_{t-1},\theta_*^+)P(y_t^-|y_{t-1},\theta_*^-)p(\theta_*^+,\theta_*^-)}
            {P(y_t^+|y_{t-1},\theta_{m-1}^+)P(y_t^-|y_{t-1},\theta_{m-1}^-)p(\theta_{m-1}^+,\theta_{m-1}^-)}
            \frac{P(y_{ex}^+|y_{t-1},\theta_{m-1}^+)P(y_{ex}^-|y_{t-1},\theta_{m-1}^-)}{P(y_{ex}^+|y_{t-1},\theta_*^+)P(y_{ex}^-|y_{t-1},\theta_*^-)}\]
    \item With probability $min(\pi,1)$, accept the proposal and put $(\theta_m^+,\theta_m^-) = (\theta_*^+,\theta_*^-)$.\\
        Otherwise, reject the proposal and put $(\theta_m^+,\theta_m^-) = (\theta_{m-1}^+,\theta_{m-1}^-)$.
\end{enumerate}
\end{algo}    
Observe that, the $\pi$ has no normalizing constant terms because they are canceled out by added exchange terms.
\(log\pi = (\theta_*^+-\theta_{m-1}^+)(s(y_{t}^+,y_{t-1})-s(y_{ex}^+,y_{t-1}))
+(\theta_*^- -\theta_{m-1}^-)(s(y_{t}^-,y_{t-1})-s(y_{ex}^-,y_{t-1}))+log \frac{P(\theta_*^+,\theta_*^-)}{P(\theta_{m-1}^+,\theta_{m-1}^-)}\)
\( = (\theta_*^+-\theta_{m-1}^+)(s'(y_{t}^+)-s'(y_{ex}^+))
+(\theta_*^- -\theta_{m-1}^-)(s'(y_{t}^-)-s'(y_{ex}^-))+log \frac{P(\theta_*^+,\theta_*^-)}{P(\theta_{m-1}^+,\theta_{m-1}^-)}\)


We need one more thing. To generate the exchange graph $y_{ex}$ for time $t$ (of the second part of main chain), 
we should have a generative algorithm at a given parameter points.
However, we still do not know the normalizing constant. Thus, I use one more MCMC chain.
\begin{algo}[the auxiliary chain]
Let $\theta^+,\theta^-,y_{t-1}$ be given. For $k=1,...,K$,
\begin{enumerate}
    \item Select one edge randomly from the $y_{k-1}$, say, $y_{k-1;ij}$.
    \item Propose a new graph $y_*$ with switching the $y_{ij}$ value from $y_{k-1}$: 
    \\ If $y_{k-1;ij}=1$, then set $y_{*;ij}=0$(dissolution case.) Oterwise, If $y_{k-1;ij}=0$, then set $y_{*;ij}=1$(formation case.)
    \\ (If sample graphs are undirected, switch $y_{ji}$ simultaneously.)
    \item Take $\theta$ as $\theta^+$ or $\theta^-$ according to the case. Calculate the MCMC ratio $\phi$,
    \[\phi = \frac{P(Y_t=y_*|y_{t-1},\theta)}{P(Y_t=y_{k-1}|y_{t-1},\theta)}= \frac{exp(\theta^T s(y_*,y_{t-1}))}{exp(\theta^T s(y_{k-1},y_{t-1}))} = exp(\theta^T (s'(y*)-s'(y_{k-1})))\]
    \item With probability $min(\phi,1)$, accept the proposal and put $y_k=y_*$.\\
        Otherwise, reject the proposal and put $y_k=y_{k-1}$.
\end{enumerate}
\end{algo}
After $K$ iteration, use the last network as the exchange sample in the second part of the main algorithm.


\section{Diagnostics, inference, and Bayesian GOF}



\subsection{Model diagnostics}
Since we run two kinds of MCMC chains, we should proceed two diagnostic task.

The main chain produces the posterior samples of parameter, so the procedure is same as ordinary MCMC case.
\begin{itemize}
    \item Cut burn-in period. Do thinning if it is needed.
    \item Depict traceplots of each parameter chain to check the convergence and the mixing.
    \item Depict autocorrelation plot. Calculate ESS if it is needed.
\end{itemize}

Next, checking all auxiliary chains is practically irritating one. 
In general, it is suffice to check the auxiliary chain of the last iteration (of the main chain)
with statistics included in the model.
\begin{itemize}
    \item Calculate network statistics of all graphs produced by the last auxiliary chain.
    \item Depict traceplots of each statistics to check the convergence and the mixing.
\end{itemize}



\subsection{Model inference}

Since we have posterior samples by running the main chain as many as we want, we can do basic inference procedure.
\begin{itemize}
    \item A outlining shape of the posterior of $\theta^+,\theta^-|y_1,y_2,...,y_T$ by histogram.
    \item An approximated summary statistics: mean, mode, variance, ...
    \item An approximated quantile and probability interval
\end{itemize}

Moreover, we already have a generative algorithm at the specific parameter point,
we can predict the form of network at $T+1,T+2,...$ using posterior sample following standard Bayesian method.
For example, for predicting $T+1$,
\begin{itemize}
    \item Run K iteration using auxiliary chain at $y_T$ with each posterior sample points.
    \item Take the each network as a predicted result.
\end{itemize}
If you need, calculate some network statistics for the results and get a summary statistics of them.


\subsection{Goodness of Fit}
To evaluate the goodness of fit for the posterior $\theta^+,\theta^-$,
we can use the auxiliary chain algorithm once again.
\begin{algo}[GOF procedure]
    For t=2,...,T
    \\For s=1,...,S
    \begin{enumerate}
        \item sample $\theta_s^+,\theta_s^-$ from the estimate of posterior.
        \item simulate $y_s$ using the auxiliary chain under $y_{t-1}$.
        \item calculate $g(y_s)$, some higher degree statistics (eg. Node-degree dist \& Edgewise Shared Partner dist)
    \end{enumerate}
    Draw the box-plot of $g(y_s)$ and compare with $g(y_t)$.
\end{algo}


\section{Example: Scholars and faculty members' experiences
in online social networks}
\section{Summary and Discussion}







\begin{frame}{Supplements}
    You can find the C++ implementation (using Armadilo: see http://arma.sourceforge.net/)
    \\ of BSTERGM fitting, diagnostic, and GOF algorithms
    \\ at my Github page: https://github.com/letsjdosth/BayesianSTERGM.
\end{frame}


testcite \cite{RN100}

\nocite{*}
\bibliographystyle{plain}
\bibliography{doc.bib}

\end{document}
